Let’s do a single, detailed training step from start

We’ll use a triplet loss example (anchor / positive / negative).

We’ll show:

1.  forward pass → embeddings → similarities → loss
2.  exact gradients of the loss w.r.t. the embeddings (derivation + numbers)
3.  one gradient-descent parameter update on the embeddings (and also show how this maps back to a linear layer `W`)
4.  recompute similarities and loss to see the effect

## Setup

Inputs (raw vectors / “initial embeddings” we will treat as the model outputs for simplicity):

- Anchor $a = [1.0,0.0]$ (sentence: “I like dogs”)
- Positive $p = [0.1,0.4]$ (sentence: “I enjoy puppies”)
- Negative $n = [0.0, 1.0]$ (sentence: “Quantum physics is hard”)

We will use triplet loss (margin form):

$$
    Loss = max(0, cos(\alpha,n) - cos(\alpha, p) + margin)
$$

Choose margin $m = 0.5$ (This is purposely large so the initial loss is positive and we can see movement.)

We treat the three vectors $a, p, n$ as the current embeddings (in practice they come from a network; later we’ll show how to update a linear weight matrix $W$).

1. Forward pass — compute cosines and initial loss

$$
    Loss_{before} = max(0, 0.0 - 0.2425356250 + 0.5) = 0.2474643750.
$$

So initially the model is penalized `(loss ≈ 0.25746)` — positive and negative aren’t separated by the margin yet.

2. Compute gradients (how to move embeddings to reduce loss)

Because the loss is positive, the inside of the max is active. So

$$
    L = cos(\alpha, n) - cos(\alpha, p) + m
$$

We need gradients w.r.t. the three embeddings $a, p, n$.

For vectors $x,y$,

$$
    cos(x, y) = \frac {x.y}{||x||\ ||y||}
$$

<!-- TODO: learn differentiation first -->
