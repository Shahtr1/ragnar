Right now:

- We have embeddings (vectors for sentences).
- Cosine similarity tells us how close they are in meaning.

But unless we train the model, the vectors might not line up semantically.

- Training makes sure “I like dogs” and “I enjoy puppies” end up near each other, while “The stock market crashed” is far away.

How do we do that?
We use loss functions that literally pull good pairs closer and push bad pairs apart.

## 1. Contrastive Loss (the simplest)

Think of pairs of sentences:

- Positive pair = should be close.
- Negative pair = should be far.

Tiny numeric example

Let’s imagine we have embeddings:

- u = `[1, 0]` = "i like dogs"
- $v^+$ = `[0.9, 0.1]` = "i enjoy puppies" (positive)
- $u^-$ = `[0, 1]` = "Quantum physics is hard" (negative)

### Step 1 — Cosine similarities

- $cos(u, v^+) \approx 0.99$ (very close)
- $cos(u, v^-) = 0$ (orthogonal)

### Step 2 — Loss behavior

- For positive pairs, loss wants cosine → 1 (pull them closer).
- For negatives, loss wants cosine < margin (push them apart).

If margin = 0.2:

- $cos(u, v^+)$ = 0.99 → good (close enough).
- $cos(u, v^-)$ = 0 → already less than 0.2 → also good.

Loss = small → model is happy.

## 2. Triplet Loss (anchor, positive, negative)

Here we look at three sentences at once:

- Anchor = “I like dogs” → vector a = `[1, 0]`
- Positive = “I enjoy puppies” → p = `[0.9, 0.1]`
- Negative = “Quantum physics is hard” → n = `[0, 1]`

$$
    Loss = max(0, cos(\alpha,n) - cos(\alpha, p) + margin)
$$

- Make sure anchor–positive similarity is at least margin bigger than anchor–negative similarity.

Step by step

- cos(a, p) ≈ 0.99
- cos(a, n) = 0
- margin = 0.2

$$
    Loss = max(0, 0 - 0.99 + 0.2) = max(0, -0.79) = 0.
$$

- So no loss → model is satisfied.
- If anchor–positive was too weak, loss would > 0 → model adjusts weights to fix it.

## Intuition

Training is literally reshaping the map of embeddings:

- Pulling similar sentences (like dogs & puppies) closer (smaller angle).
- Pushing unrelated sentences (dogs vs physics) further apart (larger angle)
