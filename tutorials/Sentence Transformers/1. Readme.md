## Why we need sentence embeddings?

Imagine every sentence becomes a point in space (a vector). Sentences that mean the same thing are close together; sentences that mean different things are far apart. If you can do that, you can:

- Find similar sentences fast (semantic search).
- Cluster documents by meaning.
- Detect paraphrases.
- Use embeddings as numeric features for downstream models.

## 1) From raw text to numbers (tokenization → embeddings)

Before any neural network can work on text, text must become numbers.

Example sentence:
`"I love apples"`

Tokenization (BPE / WordPiece)

- The tokenizer splits string into tokens (subword units). Example (illustrative, not exact): `["I", "love", "ap", "ples"]`
- Each token maps to an integer id: e.g. `I → 10, love → 423, ap → 301, ples → 402`.

### Why integers?

Because computers don’t work with text, they only work with numbers (binary). We can’t directly feed `"love"` into a neural net. But if we assign `"love"` an ID (say `423`), then we can look it up in a table of vectors (an embedding matrix).

### Embedding lookup (turn IDs into vectors)

Now, suppose we have an embedding matrix:

$$
E \in \mathbb{R}^{V \times d}
$$

- $V$ = vocabulary size (say 30,000 tokens).
- $d$ = embedding dimension (say 768 in BERT, or 4 in toy example).

Each row = a learned vector for one token.
Example (tiny d = 4):

| Token  | ID  | Embedding vector       |
| ------ | --- | ---------------------- |
| "I"    | 10  | \[0.1, 0.2, 0.0, 0.5]  |
| "love" | 423 | \[0.3, 0.4, -0.1, 0.2] |
| "ap"   | 301 | \[0.0, 0.2, 0.1, 0.0]  |
| "ples" | 402 | \[0.1, 0.0, 0.1, -0.1] |

### How does the model decide what numbers go into those vectors (rows of the embedding matrix)?

1. At the very beginning (before training)

- The embedding matrix $ E \in \mathbb{R}^{V \times d} $ is created.
- Every entry is usually just random numbers (tiny random floats, like 0.01, -0.03, …).
- So initially, `"I"`, `"love"`, `"ap"`, `"ples"` have random vectors — no meaning yet.

At this stage the network knows nothing about language.

2. Training with a task

The model then trains on a task — like predicting the next word.
Example sentence from dataset:

`"I love apples"`

Task: predict the next token given the previous ones.

- Input: `"I love"`
- Target: `"apples"`

So the network will:

- Take token IDs → look up their random embeddings → process with transformer layers.
- Try to predict `"apples"` as the next token.
- Compare its guess with the true answer using loss (cross-entropy).
- Backpropagate the error → update weights, including the embeddings.

3. Updating embeddings with gradient descent

Here’s the key:
When the model is wrong, gradients flow back all the way to the embedding matrix.

Example:

- "love" was used in the input.
- The prediction was bad.
- Backprop says: “hey, the row for `"love"` should shift slightly so that next time, the network predicts better.”

So the vector for "love" (row 423 in $E$) gets nudged:

old vector: `[0.3,0.1,−0.2,0.5]`
new vector: `[0.31,0.12,−0.19,0.49]`

_“The embedding matrix is a group of vectors, each vector has dimensions. The 4th dimension of the vector for `‘love’` decreased slightly `(0.5 → 0.49)` because backpropagation decided that lowering it reduces the model’s error.”_

Do this billions of times across a giant text dataset → embeddings move into positions that capture semantic meaning.

4. What ends up happening?

Through training:

- Words that appear in similar contexts get vectors close to each other.
  (e.g. `"cat"` and `"dog"` appear in sentences like “The \_\_\_ is sleeping.”)
- Directions in the vector space start representing relationships.
  `"king - man + woman ≈ queen"`

So embeddings are not hand-crafted — they are learned automatically through the task + gradient descent.

#### Forward pass (one run of the model):

When you give the model the sentence `"I love apples"`, here’s what happens immediately:

Token → embedding
Each token ID (like `"I" → 10, "love" → 423`) is looked up in the embedding matrix.
Example vectors:

- `"I"` → `[0.1, 0.2, 0.0, 0.5]`
- `"love"` → `[0.3, 0.4, -0.1, 0.2]`

If we only used token embeddings, the model would know what the words are, but not their order.

Example:

- `"I love apples"` → vectors for `["I", "love", "apples"]`
- `"apples love I"` → the same vectors, just in a different sequence.

Without extra info, the model can’t easily distinguish these.

The solution: positional embeddings

We give each position in the sequence its own vector.

- Position 0 (first word in the sentence) → vector like `[0.05, -0.01, 0.02, 0.03]`
- Position 1 (second word) → vector like `[-0.02, 0.01, 0.04, 0.00]`
- Position 2 → another vector … and so on.

### How they’re used

When processing each token:

$ final token vector = token embedding + positional embedding $

Example:

- Token `"I"` → `[0.1, 0.2, 0.0, 0.5]`
- Position 0 vector → `[0.05, -0.01, 0.02, 0.03]`
- Add them: `[0.15, 0.19, 0.02, 0.53]`

So now the vector encodes both meaning and order.

## 2. Transformers — the core: self-attention

### Shape bookkeeping

Think of self-attention as a soft, learned lookup that lets each token ask the whole sentence: “who should I pay attention to, and how much?”

- Each token becomes three things simultaneously: a query (what it’s asking), a key (what it is as something others can match against), and a value (the information to be retrieved).
- For each token (a query) we compute similarity scores between that query and every key (including itself). Those scores become weights (via softmax). The token’s new representation is the weighted sum of the values.

Each token must figure out which other tokens are relevant to it. To do that, it needs a measure of similarity between itself (as a query) and the others (as keys).

The dot product is a natural similarity measure in vector spaces:

- If two vectors point in the same direction → large positive dot product.
- If they are orthogonal (unrelated) → dot product near zero.
- If they point in opposite directions → negative (in some attention variants this can mean dissimilarity).

So:

$$
    s_{ij} = q_i \cdot k_j
$$

tells us how much token i (as a query) should care about token j (as a key).

### What is "batching"?

If we process one sentence at a time, it’s slow. Computers like doing many things in parallel.

So we often feed, say, 2 sentences at once. Example:

- Sentence A: `"I love cats"` → 3 tokens.
- Sentence B: `"Birds fly"` → 2 tokens.

To make them fit together, we pad the shorter one so both look length 3 (fill with dummy tokens).

So the batch might look like this (2 sentences × 3 tokens × 4 numbers per token):

```lua
Sentence A: [[0.1, 0.2, 0.3, 0.4],
             [0.5, 0.1, 0.7, 0.9],
             [0.0, 0.6, 0.4, 0.8]]

Sentence B: [[0.3, 0.4, 0.2, 0.5],
             [0.6, 0.1, 0.3, 0.9],
             [0.0, 0.0, 0.0, 0.0]]   <-- padding

```

Shape = `(2, 3, 4)` → (2 sentences in the batch, each with 3 tokens, each token is 4 numbers).

### What is a "tensor"?

A tensor is just the general word for an array of numbers with shape.

- A list like `[2,5,7]` → tensor of shape `(3)` (1D).
- A table like `[[2,5,7],[1,0,4]]` → tensor of shape `(2,3)` (2D).
- The batch example `(2,3,4)` is a 3D tensor.

So we have:

- Tokens = pieces of text.
- Each token → vector of numbers.
- Stack tokens → 2D array (tokens × features).
- Stack sentences → 3D array (batch × tokens × features).
- Tensor = just the word for "this shaped box of numbers."

### What is an "attention head"?

Imagine you’re reading the sentence:
“The dog chased the ball because it was fast.”

Question: what does “it” refer to?

- Could be the dog.
- Could be the ball.

When humans read, our brain connects “it” back to either “dog” or “ball” depending on context.

An attention head is like a little "spotlight" inside the model.

- It looks at one token (say “it”) and then decides how much to pay attention to other tokens (dog, ball, etc).
- The model learns these connections automatically.
- Some heads specialize in subject-verb links, others in pronouns, others in long-distance relations.
- Each head is a little detective with its own "focus style."

### How many heads?

In practice, we don’t rely on just one spotlight.

- If we had only one, it might always pick “dog” and miss “ball.”
- Instead, Transformers use multiple heads (say 8 or 12).

Each one looks at the same sentence but from a different angle.

Example:

- Head 1: focuses on subject–verb grammar.
- Head 2: focuses on pronouns.
- Head 3: focuses on nearby words.
- … and so on.

At the end, all the heads combine their findings into one big understanding.

### How is an attention head built?

Okay, now let’s tie it back to shapes.

We already said:

- Input tensor = `(Batch, Tokens, Features)` = `(B, T, d_model)`.

For each head, the model projects (transforms) the features into 3 roles:

- Query = “what am I looking for?”
- Key = “what do I offer to others?”
- Value = “the actual content I carry.”

Every token becomes a (Query, Key, Value) triple.

Each of these is just another tensor (a reshaped set of numbers).
The shapes will soon make sense once we set concrete numbers.

### Starting point: the input tensor

Say we have:

- Batch size B = 2 (two sentences at once).
- Tokens T = 3 (each sentence has 3 tokens).
- Features d_model = 4 (each token is represented by 4 numbers).

So the input tensor X has shape:
`(B, T, d_model)` = `(2, 3, 4)`

```less
Sentence A (3 tokens × 4 numbers):
[ [0.1, 0.2, 0.3, 0.4],   # token 1
  [0.5, 0.1, 0.7, 0.9],   # token 2
  [0.0, 0.6, 0.4, 0.8] ]  # token 3

Sentence B (3 tokens × 4 numbers):
[ [0.3, 0.4, 0.2, 0.5],
  [0.6, 0.1, 0.3, 0.9],
  [0.0, 0.0, 0.0, 0.0] ]   # padding token
```

### One attention head

Suppose we want 1 head only.

This head will create three new views of the data:

- Q (Queries)
- K (Keys)
- V (Values)

Each of these is made by multiplying `X` with a learned weight matrix.

- Think of it like putting glasses on: the same tokens are seen in a slightly different way.
- Mathematically, each weight matrix is just a table of numbers the model learns.

- X → three new views (Q, K, V)

We need each token to play 3 roles:

- Query (Q): "What am I looking for in others?"
- Key (K): "What do I offer to others?"
- Value (V): "What content should be shared if I’m chosen?"

To get these, we multiply `X` by three separate weight matrices:

- `Q = X @ W_Q`
- `K = X @ W_K`
- `V = X @ W_V`

These weight matrices also start random and are learned during training, just like the embedding matrix.

### Why multiple dimensions are powerful

- A query with 1 number can only measure similarity on 1 trait.
- A query with 2+ numbers can measure similarity across multiple traits.
- That’s why we pick `d_k > 1`.

### Why not make d_k huge then?

If `d_k` is too large, dot products `QK^T` blow up in size → softmax saturates → training becomes unstable (this is why we divide by `√d_k`).

### Why multiply with a matrix?

Think of your token vector x (say `[0.1, 0.2, 0.3, 0.4]`).
It’s just 4 numbers, but those numbers are raw features from embeddings: a mix of meaning, position, etc.

Now we want to turn this single vector into a query (Q).
But how do we decide which combination of those 4 features makes a good query?

By multiplying with a matrix.

- A matrix multiplication lets us take linear combinations of the input features.
- Each row of the weight matrix W_Q says: “to get this new feature, combine the old features in this specific way.”

```lua
x = [0.1, 0.2, 0.3, 0.4]   (1×4)
W_Q = [[1, 0],
       [0, 1],
       [1, 1],
       [0, -1]]   (4×2)

Q = x @ W_Q = [0.1*1 + 0.2*0 + 0.3*1 + 0.4*0,   0.1*0 + 0.2*1 + 0.3*1 + 0.4*(-1)]
             = [0.4, 0.1]

```

### Why not just dot product?

A dot product only works between two vectors of the same size.

```js
[0.1, 0.2, 0.3, 0.4] ⋅ [1, 0, 0, -1] = 0.1*1 + 0.2*0 + 0.3*0 + 0.4*(-1) = -0.3
```

That gives just one number.
But we need a new vector (like 2 numbers, 64 numbers, 128 numbers depending on head size), not just a single score.

So instead of dotting with one vector, we “dot” with many vectors stacked together — that’s exactly what a matrix is: a stack of vectors!

- Each column of `W_Q` is like a different “lens” or “filter” we apply.
- Multiplying `x` by `W_Q` applies all those filters at once and produces multiple numbers.

### Why is this powerful?

- If we just passed X directly, attention would always measure similarity in the same space.
- By learning W_Q and W_K, the model gets to redefine what similarity means.

  - Example: one head might learn “similarity in terms of grammatical role.”
  - Another head might learn “similarity in terms of semantic meaning.”

- And W_V decides what actual content gets passed along once attention is decided.
